
#!/bin/bash

echo "ğŸ”§ Installing llama.cpp on Remote Server"
echo "========================================"

# Update system
echo "ğŸ“¦ Updating system packages..."
sudo apt-get update

# Install dependencies
echo "ğŸ› ï¸ Installing build dependencies..."
sudo apt-get install -y build-essential cmake git python3 python3-pip

# Install Python dependencies
echo "ğŸ Installing Python dependencies..."
pip3 install requests websockets

# Clone and build llama.cpp
echo "ğŸ“¥ Cloning llama.cpp..."
cd /tmp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

echo "ğŸ”¨ Building llama.cpp..."
mkdir build
cd build
cmake .. -DLLAMA_CUDA=OFF
make -j$(nproc)

echo "âœ… llama.cpp installed successfully!"
echo "ğŸ“ Binary location: /tmp/llama.cpp/build/bin/llama-server"

# Make scripts executable
chmod +x /tmp/llama.cpp/build/bin/*

echo ""
echo "ğŸ‰ Installation Complete!"
echo "ğŸ“ Next steps:"
echo "1. Download your GGUF model to the remote server"
echo "2. Run the remote_llm_server.py script"
echo ""
